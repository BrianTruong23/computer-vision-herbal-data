{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvL0ubHcHfK9"
      },
      "source": [
        "\n",
        "## Training and Pruning the ResNet 8 model to fit into the mobile size device.\n",
        "\n",
        "**Project's Objective:**The project involves the traning and pruning of the model to integrate into a mobile application. The project aims to provide a good and compact image classification model to let people use it without relying on the server or internet connection. It will be really useful for Afrian people in remote places to use the app and utilize the power of machine learning for their tree-related activities (e.g gardening, harvesting, planting and using trees for herbal use).\n",
        "\n",
        "I am very excited to work on this project for several reasons:\n",
        "- 1) I can learn the architecture of the ResNet 8 for image classification. I will be involved pretty heavily into training, pruning and integrating the model into mobile application. The last stage is something I have never done before.\n",
        "- 2) I can learn about the field of herbal medicine and the federated learning. This will be the first step into building the federated learning architecture where the weights of the model will be sent to the mobile application and the data will be trained within the mobile apps instead of sending that sensitive data back to the server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQCL688c_A5f"
      },
      "source": [
        "### First Stage: The first stage involves loading the data, understand the ResNet 8 architecture and train the model with that architecture. (Everything will be in the Jupyter Notebook)\n",
        "\n",
        "- Loading the data from google drive. ✅\n",
        "- Understand ResNet 8 architecture\n",
        "  [Resource 5min](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8)\n",
        "- Using RestNet 8 for feature extraction ✅\n",
        "- Doing dimensionality reduction with autoencoder\n",
        "- Applying random forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BID7Sv1RAvT1",
        "outputId": "a67824e8-6154-4415-a3bd-1361327d2ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = '/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/Thang_folder/Herb_Images_Dataset_shortlisted'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th5zgx0DAxRV",
        "outputId": "8c55f9f5-d706-473e-a56b-e6a16c74d9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_io.TextIOWrapper name='/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/Thang_folder/Herb_Images_Dataset_shortlisted/Acacia senegal/1-s2.0-S0268005X17306112-egi10HGQM8J72N.jpg' mode='r' encoding='UTF-8'>\n"
          ]
        }
      ],
      "source": [
        "with open(f'{gdrive_path}/Acacia senegal/1-s2.0-S0268005X17306112-egi10HGQM8J72N.jpg') as file:\n",
        "  print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ProrZADedSK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "image_folder = gdrive_path\n",
        "required_images = 190\n",
        "\n",
        "# Create a dataset\n",
        "dataset = datasets.ImageFolder(image_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-StCwDGVejex"
      },
      "source": [
        "⚠️⚠️⚠️Uncomment this block if you want to experiment with ResNet 50. The model architecture takes a long time to render but gives a low accuracy ⚠️⚠️⚠️\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "_M3CVnkiAxcx",
        "outputId": "86e08cad-efa5-4f52-851c-9258322be0f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 87.7MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cead13e6d75e>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mfeatures_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeatures_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to extract any features.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-cead13e6d75e>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i}: images tensor has incorrect dimensions. Shape: {images.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in batch {i}: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torchvision.models as models\n",
        "# import os\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# dataset.transform = transform\n",
        "\n",
        "# # Load pre-trained ResNet-50\n",
        "# resnet = models.resnet50(pretrained=True)\n",
        "# input_size = 224  # ResNet-50 input size\n",
        "\n",
        "# # Remove the last layers (including the average pooling layer)\n",
        "# modules = list(resnet.children())[:-2]\n",
        "\n",
        "# # Create a new model\n",
        "# feature_extractor = torch.nn.Sequential(*modules)\n",
        "\n",
        "# # Add new layers\n",
        "# feature_extractor.add_module('avg_pool', torch.nn.AdaptiveAvgPool2d((1, 1)))\n",
        "# feature_extractor.add_module('flatten', torch.nn.Flatten())\n",
        "# feature_extractor.add_module('fc', torch.nn.Linear(2048, len(dataset.classes)))\n",
        "# feature_extractor.add_module('softmax', torch.nn.Softmax(dim=1))\n",
        "\n",
        "# # Function to extract features\n",
        "# def extract_features(dataset):\n",
        "#     loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "#     features = []\n",
        "#     with torch.no_grad():\n",
        "#         for i, (images, _) in enumerate(loader):\n",
        "#             try:\n",
        "#                 if not isinstance(images, torch.Tensor):\n",
        "#                     print(f\"Batch {i}: images is not a tensor. Type: {type(images)}\")\n",
        "#                     continue\n",
        "#                 if images.dim() != 4:\n",
        "#                     print(f\"Batch {i}: images tensor has incorrect dimensions. Shape: {images.shape}\")\n",
        "#                     continue\n",
        "#                 features.append(feature_extractor(images))\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error in batch {i}: {str(e)}\")\n",
        "#     return torch.cat(features, dim=0) if features else None\n",
        "\n",
        "# # Extract features\n",
        "# features_train = extract_features(dataset)\n",
        "# if features_train is None:\n",
        "#     print(\"Failed to extract any features.\")\n",
        "# else:\n",
        "#     print(f\"Successfully extracted features. Shape: {features_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RqacSvjolxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "d138e7bc-2c6e-4e87-df3b-42f99c760f16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-16396f45d1a9>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Save the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'features_train' is not defined"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torchvision.models as models\n",
        "# import os\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "# output_folder = '/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/features_train_update2.pt'\n",
        "\n",
        "# # Save the features\n",
        "# torch.save(features_train, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the feature extraction from google drive"
      ],
      "metadata": {
        "id": "v9OKzpGzzYyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kusPvHa-dvPe",
        "outputId": "384cfb39-8712-401e-da49-e4c41675b051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features loaded. Shape: torch.Size([5623, 29])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "output_folder = '/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/features_train.pt'\n",
        "\n",
        "# Load the saved features\n",
        "import torch\n",
        "loaded_features = torch.load(output_folder)\n",
        "print(f\"Features loaded. Shape: {loaded_features.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_JFZ_MifIfq",
        "outputId": "56fd555e-f735-4f0f-8654-cdda69ef61df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 25.69%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare labels\n",
        "labels = [label for _, label in dataset]\n",
        "\n",
        "# Divide data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(loaded_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an ensemble of classifiers (Random Forest in this case)\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Test the classifier\n",
        "predicted_labels = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd5-7lOEqqld"
      },
      "source": [
        "Pipeline for training again the ResNet 18 architecture and testing with Random Forest Classifer\n",
        "\n",
        "- 1) Define a simple ResNet 18\n",
        "- 2) Use Random Forest Classifier to predict\n",
        "- 3) Modify ResNet 8 with more complicated architecture and transfer learning\n",
        "- 4) Increase the number of trees\n",
        "- 5) Pruning and Quantization to fit into a mobile app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA6Zr8ESr6X9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def trainResNet8(dataset, name):\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    dataset.transform = transform\n",
        "\n",
        "    # Define Custom ResNet-8 Model\n",
        "    class CustomResNet(nn.Module):\n",
        "        def __init__(self, num_classes):\n",
        "            super(CustomResNet, self).__init__()\n",
        "            resnet = models.resnet18(pretrained=True)\n",
        "            modules = list(resnet.children())[:-2]\n",
        "            self.feature_extractor = nn.Sequential(*modules)\n",
        "            self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.flatten = nn.Flatten()\n",
        "            self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.feature_extractor(x)\n",
        "            x = self.avg_pool(x)\n",
        "            x = self.flatten(x)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    num_classes = len(dataset.classes)\n",
        "    model = CustomResNet(num_classes=num_classes)\n",
        "\n",
        "    # Fine-tune the last layer\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    num_epochs = 5\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, labels in dataloader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "    # Extract features\n",
        "    features_train = extract_features(model, dataset)\n",
        "    if features_train is None:\n",
        "        print(\"Failed to extract any features.\")\n",
        "    else:\n",
        "        print(f\"Successfully extracted features. Shape: {features_train.shape}\")\n",
        "\n",
        "    saveResNet8(torch, name, features_train)\n",
        "\n",
        "    return features_train\n",
        "\n",
        "def extract_features(model, dataset):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(loader):\n",
        "            try:\n",
        "                if not isinstance(images, torch.Tensor):\n",
        "                    print(f\"Batch {i}: images is not a tensor. Type: {type(images)}\")\n",
        "                    continue\n",
        "                if images.dim() != 4:\n",
        "                    print(f\"Batch {i}: images tensor has incorrect dimensions. Shape: {images.shape}\")\n",
        "                    continue\n",
        "                features.append(model(images))\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {i}: {str(e)}\")\n",
        "    return torch.cat(features, dim=0) if features else None\n",
        "\n",
        "def saveResNet8(torch, name, features_train):\n",
        "    output_folder = f'/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/{name}.pt'\n",
        "    torch.save(features_train, output_folder)\n",
        "\n",
        "def randomForestClassifierPrediction(dataset, loaded_features):\n",
        "    # Prepare labels\n",
        "    labels = [label for _, label in dataset]\n",
        "\n",
        "    # Divide data into training and validation sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(loaded_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train an ensemble of classifiers (Random Forest in this case)\n",
        "    classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Test the classifier\n",
        "    predicted_labels = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predicted_labels)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "def svm_classifier(dataset, loaded_features):\n",
        "    # Prepare labels\n",
        "    labels = [label for _, label in dataset]\n",
        "\n",
        "    # Divide data into training and validation sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(loaded_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the SVM classifier\n",
        "    classifier = SVC(kernel='linear', random_state=42)  # You can change the kernel to 'rbf', 'poly', etc., as needed\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Test the classifier\n",
        "    predicted_labels = classifier.predict(X_test)\n",
        "\n",
        "    # Find accuracy\n",
        "    accuracy = accuracy_score(y_test, predicted_labels)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    return classifier, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8EvhN_ftBxr",
        "outputId": "9f90e9f7-5f15-4836-b027-eac99f56ae7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = '/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/Thang_folder/Herb_Images_Dataset_shortlisted'\n",
        "\n",
        "image_folder = gdrive_path\n",
        "required_images = 190\n",
        "\n",
        "# Create a dataset\n",
        "dataset = datasets.ImageFolder(image_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA_YG99os72x",
        "outputId": "8c86296d-4951-46e7-8962-4323c0a65d7d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.82%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9582222222222222"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Usage usage\n",
        "name = 'features_train_update2'\n",
        "# comment the next line if you don't have the loaded features extracted\n",
        "# features_train = trainResNet8(dataset, name)\n",
        "loaded_features = torch.load(f'/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/{name}.pt')\n",
        "randomForestClassifierPrediction(dataset, loaded_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPN1Ow2AMQuE",
        "outputId": "d5f99c44-0513-4503-d059-458cbea37ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 97.78%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9777777777777777"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "name = 'features_train_update2'\n",
        "# features_train = trainResNet8(dataset, name)\n",
        "loaded_features = torch.load(f'/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/{name}.pt')\n",
        "svm_classifier(dataset, loaded_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXAviMfcZF93",
        "outputId": "11853339-1dcf-4f5d-f4b8-04cff6158121"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 97.78%\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "name = 'features_train_update2'\n",
        "# features_train = trainResNet8(dataset, name)\n",
        "loaded_features = torch.load(f'/content/gdrive/My Drive/Summer 2024/john hopskin/Training Herbal Data/model/{name}.pt')\n",
        "classifier, accuracy = svm_classifier(dataset, loaded_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqUfJrkLcrkx"
      },
      "source": [
        "Here are the steps to do the next stage: convert and save tensorflow lite for IOS app integration:\n",
        "\n",
        "\n",
        "### Future Use\n",
        "- Save the classifier: This step is needed because we need to save the classifier for future use.\n",
        "\n",
        "\n",
        "### Conversion to Mobile Stage\n",
        "The main function of these step is to let the IOS app can execute the code. The tensorflow approach can help to integrate both in IOS and Android. However, if there is any problem with this approach, we can just shift from implementing Tensorflow Lite to  CoreML's approach which only works for IOS.\n",
        "\n",
        "#### Approach 1\n",
        "- Convert the SVM classifier to Tensorflow Model\n",
        "\n",
        "- Convert the Tensorflow Model to Tensorflow Lite model\n",
        "\n",
        "- Save the tensorflow lite\n",
        "\n",
        "#### Approach 2\n",
        "- Convert to Omnx\n",
        "\n",
        "- Convert from omnx to core ml\n",
        "\n",
        "### Import Stage\n",
        "- Import into IOS App\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub35LDZbbn7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7ac3a1-932e-48a4-e33f-aca9693c7f03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Save the classifier\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(classifier, 'svm_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('svm_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uT_JOye_qSU8",
        "outputId": "8f953818-8e4b-4a65-a439-79bdf79ceeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3aba102b-f1dc-44a4-811f-6c434aef613e\", \"svm_model.pkl\", 1078271)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the function to convert the SVM model to a TensorFlow model\n",
        "def svm_to_tf_model(classifier, input_shape):\n",
        "    # Create a Sequential model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(units=1, activation='linear', input_shape=input_shape)\n",
        "    ])\n",
        "\n",
        "    # Average the weights across support vectors\n",
        "    weights = np.mean(classifier.coef_, axis=0)\n",
        "    bias = classifier.intercept_[0]\n",
        "\n",
        "    # Ensure weights shape matches the expected input shape\n",
        "    print(f\"Input shape: {input_shape}\")\n",
        "    print(f\"Weight shape after averaging: {weights.shape}\")\n",
        "\n",
        "    expected_weight_shape = (input_shape[0], 1)\n",
        "    if weights.size != expected_weight_shape[0]:\n",
        "        raise ValueError(f\"Expected weight shape {expected_weight_shape} but got {weights.shape}\")\n",
        "\n",
        "    model.layers[0].set_weights([weights.reshape(expected_weight_shape), np.array([bias])])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load your SVM model\n",
        "classifier = joblib.load('svm_model.pkl')\n",
        "\n",
        "# Define the input shape based on loaded features\n",
        "input_shape = (loaded_features.shape[1],)\n",
        "\n",
        "# Check the shapes\n",
        "print(f\"Loaded features shape: {loaded_features.shape}\")\n",
        "print(f\"SVM classifier weights shape: {classifier.coef_.shape}\")\n",
        "\n",
        "# Convert to TensorFlow model\n",
        "tf_model = svm_to_tf_model(classifier, input_shape)\n",
        "\n",
        "# Save TensorFlow model\n",
        "tf_model.save('svm_model.h5')\n",
        "\n",
        "# Convert the TensorFlow model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open('svm_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "u3wthc_WrNDu",
        "outputId": "9371acf7-57d4-45e7-dee1-99f27c2038b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded features shape: torch.Size([5623, 29])\n",
            "SVM classifier weights shape: (406, 29)\n",
            "Input shape: (29,)\n",
            "Weight shape after averaging: (29,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1da4cd3c-6701-4a5a-bd8a-5e943a2077dd\", \"svm_model.h5\", 12400)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1fa7a13a-3dae-4b53-ae3d-95fa44adcb4a\", \"svm_model.tflite\", 1204)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9R2nmGtZQNS"
      },
      "outputs": [],
      "source": [
        "# Download the TensorFlow model\n",
        "files.download('svm_model.h5')\n",
        "\n",
        "# Download the TensorFlow Lite model\n",
        "files.download('svm_model.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa5v4EGB0TVU"
      },
      "outputs": [],
      "source": [
        "# Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9YzoIyw_RiV"
      },
      "source": [
        "### Second Stage: Build the mobile application. This will be done not in Jupyter Notebook.\n",
        "\n",
        "https://chatgpt.com/c/eec94b74-1562-4597-ba23-62b96e1091a2\n",
        "\n",
        "\n",
        "Link: https://www.youtube.com/watch?v=yV9nrRIC_R0\n",
        "\n",
        "Identify the platform to develop the mobile application\n",
        "Convert the model to Tensorflow Lite\n",
        "Then integrate the TensorFlow Lite into the mobile application.\n",
        "\n",
        "\n",
        "\n",
        "General Point about integrating the model into mobile application\n",
        " - Convert TensorFlow model to TensorFlow Lite\n",
        "  - Apply TensorFlow Quantization to make the model efficient and compact to deploy on mobile.\n",
        "  - Also, utilize the on-device accelerators to make the running of AI model faster.\n",
        "\n",
        "- Think about how to integrate the model into the Flutter\n",
        "  - Apply quantization to the ResNet 8 and autoencoder\n",
        "  - Apply some conversions to the Random Forest Classifier\n",
        "  - Apply image preproccessing before feature extract and dimensionality reduction. Finally, the use of RFC in classifying. .\n",
        "\n",
        "- Build the mobile application with Flutter with simple UI/UX design with these following properties:\n",
        "    - Allow user to upload the data\n",
        "    - Then, the model will evaluate the data using the integrated ML model.\n",
        "    - The model will give back the classification results and display on the app.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}